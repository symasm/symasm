// [https://sonictk.github.io/asm_tutorial/#beatingthecompiler/acorrelationfunction]
#include <math.h>
double correlation_ref(const double x[], const double y[], int n)
{
    double sumX = 0.0;
    double sumY = 0.0;
    double sumXX = 0.0;
    double sumYY = 0.0;
    double sumXY = 0.0;
    for (int i=0; i < n; ++i) {
        sumX += x[i];
        sumY += y[i];
        sumXX += x[i] * x[i];
        sumYY += y[i] * y[i];
        sumXY += x[i] * y[i];
    }
    double covXY = (n * sumXY) - (sumX * sumY);
    double varX = (n * sumXX) - (sumX * sumX);
    double varY = (n * sumYY) - (sumY * sumY);
    return covXY / sqrt(varX * varY);
}

        .cfi_startproc
        testl   %edx, %edx
        jle     .L5
        pxor    %xmm7, %xmm7
        leal    -1(%rdx), %r8d
        xorl    %eax, %eax
        movapd  %xmm7, %xmm3
        movapd  %xmm7, %xmm6
        movapd  %xmm7, %xmm2
        movapd  %xmm7, %xmm4
        movapd  %xmm7, %xmm5
        .p2align 4,,10
        .p2align 3
.L3:
        movsd   (%rdi,%rax,8), %xmm0
        movsd   (%rsi,%rax,8), %xmm1
        movq    %rax, %rcx
        addq    $1, %rax
        movapd  %xmm0, %xmm8
        addsd   %xmm0, %xmm5
        addsd   %xmm1, %xmm4
        mulsd   %xmm0, %xmm8
        mulsd   %xmm1, %xmm0
        addsd   %xmm8, %xmm2
        movapd  %xmm1, %xmm8
        mulsd   %xmm1, %xmm8
        addsd   %xmm0, %xmm3
        addsd   %xmm8, %xmm6
        cmpq    %rcx, %r8
        jne     .L3
        movapd  %xmm5, %xmm1
        mulsd   %xmm4, %xmm1
        mulsd   %xmm5, %xmm5
        mulsd   %xmm4, %xmm4
.L2:
        pxor    %xmm0, %xmm0
        cvtsi2sdl       %edx, %xmm0
        mulsd   %xmm0, %xmm3
        mulsd   %xmm0, %xmm2
        mulsd   %xmm6, %xmm0
        subsd   %xmm1, %xmm3
        subsd   %xmm5, %xmm2
        subsd   %xmm4, %xmm0
        mulsd   %xmm2, %xmm0
        ucomisd %xmm0, %xmm7
        movapd  %xmm0, %xmm1
        sqrtsd  %xmm1, %xmm1
        ja      .L12
        divsd   %xmm1, %xmm3
        movapd  %xmm3, %xmm0
        ret
        .p2align 4,,10
        .p2align 3
.L5:
        pxor    %xmm7, %xmm7
        movapd  %xmm7, %xmm4
        movapd  %xmm7, %xmm5
        movapd  %xmm7, %xmm1
        movapd  %xmm7, %xmm3
        movapd  %xmm7, %xmm6
        movapd  %xmm7, %xmm2
        jmp     .L2
.L12:
        subq    $24, %rsp
        .cfi_def_cfa_offset 32
        movsd   %xmm3, 8(%rsp)
        movsd   %xmm1, (%rsp)
        call    sqrt@PLT
        movsd   8(%rsp), %xmm3
        movsd   (%rsp), %xmm1
        addq    $24, %rsp
        .cfi_def_cfa_offset 8
        divsd   %xmm1, %xmm3
        movapd  %xmm3, %xmm0
        ret
        .cfi_endproc

        .cfi_startproc
        test    edx, edx
        jle     .L5
        pxor    xmm7, xmm7
        lea     r8d, -1[rdx]
        xor     eax, eax
        movapd  xmm3, xmm7
        movapd  xmm6, xmm7
        movapd  xmm2, xmm7
        movapd  xmm4, xmm7
        movapd  xmm5, xmm7
        .p2align 4,,10
        .p2align 3
.L3:
        movsd   xmm0, QWORD PTR [rdi+rax*8]
        movsd   xmm1, QWORD PTR [rsi+rax*8]
        mov     rcx, rax
        add     rax, 1
        movapd  xmm8, xmm0
        addsd   xmm5, xmm0
        addsd   xmm4, xmm1
        mulsd   xmm8, xmm0
        mulsd   xmm0, xmm1
        addsd   xmm2, xmm8
        movapd  xmm8, xmm1
        mulsd   xmm8, xmm1
        addsd   xmm3, xmm0
        addsd   xmm6, xmm8
        cmp     r8, rcx
        jne     .L3
        movapd  xmm1, xmm5
        mulsd   xmm1, xmm4
        mulsd   xmm5, xmm5
        mulsd   xmm4, xmm4
.L2:
        pxor    xmm0, xmm0
        cvtsi2sd        xmm0, edx
        mulsd   xmm3, xmm0
        mulsd   xmm2, xmm0
        mulsd   xmm0, xmm6
        subsd   xmm3, xmm1
        subsd   xmm2, xmm5
        subsd   xmm0, xmm4
        mulsd   xmm0, xmm2
        ucomisd xmm7, xmm0
        movapd  xmm1, xmm0
        sqrtsd  xmm1, xmm1
        ja      .L12
        divsd   xmm3, xmm1
        movapd  xmm0, xmm3
        ret
        .p2align 4,,10
        .p2align 3
.L5:
        pxor    xmm7, xmm7
        movapd  xmm4, xmm7
        movapd  xmm5, xmm7
        movapd  xmm1, xmm7
        movapd  xmm3, xmm7
        movapd  xmm6, xmm7
        movapd  xmm2, xmm7
        jmp     .L2
.L12:
        sub     rsp, 24
        .cfi_def_cfa_offset 32
        movsd   QWORD PTR 8[rsp], xmm3
        movsd   QWORD PTR [rsp], xmm1
        call    sqrt@PLT
        movsd   xmm3, QWORD PTR 8[rsp]
        movsd   xmm1, QWORD PTR [rsp]
        add     rsp, 24
        .cfi_def_cfa_offset 8
        divsd   xmm3, xmm1
        movapd  xmm0, xmm3
        ret
        .cfi_endproc

        .cfi_startproc
        edx <= 0 : .L5
        xmm7 |=| 0
        r8i = &[rdx-1]
        eax = 0
        xmm3d |=a| xmm7
        xmm6d |=a| xmm7
        xmm2d |=a| xmm7
        xmm4d |=a| xmm7
        xmm5d |=a| xmm7
        .p2align 4,,10
        .p2align 3
.L3:
        xmm0d = 8bytes[rdi+rax*8]
        xmm1d = 8bytes[rsi+rax*8]
        rcx = rax
        rax += 1
        xmm8d |=a| xmm0
        xmm5d += xmm0
        xmm4d += xmm1
        xmm8d *= xmm0
        xmm0d *= xmm1
        xmm2d += xmm8
        xmm8d |=a| xmm1
        xmm8d *= xmm1
        xmm3d += xmm0
        xmm6d += xmm8
        r8 != rcx : .L3
        xmm1d |=a| xmm5
        xmm1d *= xmm4
        xmm5d *= xmm5
        xmm4d *= xmm4
.L2:
        xmm0 |=| 0
        xmm0d = float(edx)
        xmm3d *= xmm0
        xmm2d *= xmm0
        xmm0d *= xmm6
        xmm3d -= xmm1
        xmm2d -= xmm5
        xmm0d -= xmm4
        xmm0d *= xmm2
        xmm7d uo<=> xmm0d
        xmm1d |=a| xmm0
        xmm1d = sqrt(xmm1)
        u> : .L12
        xmm3d /= xmm1
        xmm0d |=a| xmm3
        ret
        .p2align 4,,10
        .p2align 3
.L5:
        xmm7 |=| 0
        xmm4d |=a| xmm7
        xmm5d |=a| xmm7
        xmm1d |=a| xmm7
        xmm3d |=a| xmm7
        xmm6d |=a| xmm7
        xmm2d |=a| xmm7
        :.L2
.L12:
        rsp -= 24
        .cfi_def_cfa_offset 32
        8bytes[rsp+8] = xmm3d
        8bytes[rsp] = xmm1d
        sqrt@PLT(...)
        xmm3d = 8bytes[rsp+8]
        xmm1d = 8bytes[rsp]
        rsp += 24
        .cfi_def_cfa_offset 8
        xmm3d /= xmm1
        xmm0d |=a| xmm3
        ret
        .cfi_endproc

------------------------------------------

// gcc-9 -O3 -march=haswell test.c -S

        .cfi_startproc
        testl   %edx, %edx
        jle     .L7
        leal    -1(%rdx), %eax
        cmpl    $2, %eax
        jbe     .L8
        movl    %edx, %ecx
        vxorpd  %xmm7, %xmm7, %xmm7
        xorl    %eax, %eax
        vmovapd %xmm7, %xmm8
        shrl    $2, %ecx
        vmovapd %xmm7, %xmm9
        vmovapd %xmm7, %xmm6
        salq    $5, %rcx
        vmovapd %xmm7, %xmm1
        vmovapd %xmm7, %xmm5
        .p2align 4,,10
        .p2align 3
.L4:
        vmovupd (%rdi,%rax), %ymm0
        vmovupd (%rsi,%rax), %ymm10
        addq    $32, %rax
        vaddsd  %xmm5, %xmm0, %xmm5
        vunpckhpd       %xmm0, %xmm0, %xmm2
        vaddsd  %xmm1, %xmm10, %xmm1
        vextractf128    $0x1, %ymm0, %xmm3
        vaddsd  %xmm5, %xmm2, %xmm2
        vaddsd  %xmm2, %xmm3, %xmm5
        vunpckhpd       %xmm10, %xmm10, %xmm2
        vunpckhpd       %xmm3, %xmm3, %xmm3
        vaddsd  %xmm2, %xmm1, %xmm1
        vaddsd  %xmm3, %xmm5, %xmm5
        vextractf128    $0x1, %ymm10, %xmm3
        vaddsd  %xmm3, %xmm1, %xmm2
        vunpckhpd       %xmm3, %xmm3, %xmm1
        vmulpd  %ymm10, %ymm10, %ymm3
        vaddsd  %xmm1, %xmm2, %xmm1
        vmulpd  %ymm0, %ymm0, %ymm2
        vmulpd  %ymm10, %ymm0, %ymm0
        vaddsd  %xmm2, %xmm6, %xmm6
        vunpckhpd       %xmm2, %xmm2, %xmm4
        vextractf128    $0x1, %ymm2, %xmm2
        vaddsd  %xmm4, %xmm6, %xmm6
        vaddsd  %xmm3, %xmm9, %xmm4
        vaddsd  %xmm2, %xmm6, %xmm6
        vunpckhpd       %xmm2, %xmm2, %xmm2
        vaddsd  %xmm2, %xmm6, %xmm6
        vunpckhpd       %xmm3, %xmm3, %xmm2
        vextractf128    $0x1, %ymm3, %xmm3
        vaddsd  %xmm2, %xmm4, %xmm4
        vaddsd  %xmm0, %xmm8, %xmm2
        vaddsd  %xmm3, %xmm4, %xmm4
        vunpckhpd       %xmm3, %xmm3, %xmm3
        vaddsd  %xmm3, %xmm4, %xmm9
        vunpckhpd       %xmm0, %xmm0, %xmm3
        vextractf128    $0x1, %ymm0, %xmm0
        vaddsd  %xmm3, %xmm2, %xmm2
        vaddsd  %xmm0, %xmm2, %xmm2
        vunpckhpd       %xmm0, %xmm0, %xmm0
        vaddsd  %xmm0, %xmm2, %xmm8
        cmpq    %rax, %rcx
        jne     .L4
        movl    %edx, %eax
        andl    $-4, %eax
        testb   $3, %dl
        je      .L16
        vzeroupper
.L3:
        movslq  %eax, %rcx
        vmovsd  (%rdi,%rcx,8), %xmm2
        vmovsd  (%rsi,%rcx,8), %xmm0
        leal    1(%rax), %ecx
        vfmadd231sd     %xmm2, %xmm2, %xmm6
        vfmadd231sd     %xmm0, %xmm0, %xmm9
        vaddsd  %xmm2, %xmm5, %xmm5
        vfmadd231sd     %xmm0, %xmm2, %xmm8
        vaddsd  %xmm0, %xmm1, %xmm1
        cmpl    %ecx, %edx
        jle     .L5
        movslq  %ecx, %rcx
        addl    $2, %eax
        vmovsd  (%rdi,%rcx,8), %xmm2
        vmovsd  (%rsi,%rcx,8), %xmm0
        vfmadd231sd     %xmm2, %xmm2, %xmm6
        vfmadd231sd     %xmm0, %xmm0, %xmm9
        vaddsd  %xmm2, %xmm5, %xmm5
        vfmadd231sd     %xmm0, %xmm2, %xmm8
        vaddsd  %xmm0, %xmm1, %xmm1
        cmpl    %eax, %edx
        jle     .L5
        cltq
        vmovsd  (%rdi,%rax,8), %xmm0
        vmovsd  (%rsi,%rax,8), %xmm2
        vfmadd231sd     %xmm0, %xmm0, %xmm6
        vfmadd231sd     %xmm2, %xmm2, %xmm9
        vaddsd  %xmm0, %xmm5, %xmm5
        vfmadd231sd     %xmm0, %xmm2, %xmm8
        vaddsd  %xmm2, %xmm1, %xmm1
.L5:
        vmulsd  %xmm5, %xmm1, %xmm2
        vmulsd  %xmm5, %xmm5, %xmm5
        vmulsd  %xmm1, %xmm1, %xmm1
.L2:
        vxorps  %xmm4, %xmm4, %xmm4
        vcvtsi2sdl      %edx, %xmm4, %xmm4
        vfmsub231sd     %xmm8, %xmm4, %xmm2
        vfmsub132sd     %xmm4, %xmm5, %xmm6
        vfmsub132sd     %xmm9, %xmm1, %xmm4
        vmulsd  %xmm4, %xmm6, %xmm4
        vucomisd        %xmm4, %xmm7
        vsqrtsd %xmm4, %xmm4, %xmm8
        ja      .L17
        vdivsd  %xmm8, %xmm2, %xmm0
        ret
        .p2align 4,,10
        .p2align 3
.L16:
        vzeroupper
        jmp     .L5
        .p2align 4,,10
        .p2align 3
.L7:
        vxorpd  %xmm7, %xmm7, %xmm7
        vmovapd %xmm7, %xmm1
        vmovapd %xmm7, %xmm5
        vmovapd %xmm7, %xmm2
        vmovapd %xmm7, %xmm8
        vmovapd %xmm7, %xmm9
        vmovapd %xmm7, %xmm6
        jmp     .L2
.L8:
        vxorpd  %xmm7, %xmm7, %xmm7
        xorl    %eax, %eax
        vmovapd %xmm7, %xmm8
        vmovapd %xmm7, %xmm9
        vmovapd %xmm7, %xmm6
        vmovapd %xmm7, %xmm1
        vmovapd %xmm7, %xmm5
        jmp     .L3
.L17:
        leaq    8(%rsp), %r10
        .cfi_def_cfa 10, 0
        andq    $-32, %rsp
        vmovapd %xmm4, %xmm0
        pushq   -8(%r10)
        pushq   %rbp
        movq    %rsp, %rbp
        .cfi_escape 0x10,0x6,0x2,0x76,0
        pushq   %r10
        .cfi_escape 0xf,0x3,0x76,0x78,0x6
        subq    $40, %rsp
        vmovsd  %xmm2, -32(%rbp)
        vmovsd  %xmm8, -24(%rbp)
        call    sqrt@PLT
        vmovsd  -32(%rbp), %xmm2
        vmovsd  -24(%rbp), %xmm8
        addq    $40, %rsp
        popq    %r10
        .cfi_def_cfa 10, 0
        popq    %rbp
        vdivsd  %xmm8, %xmm2, %xmm0
        leaq    -8(%r10), %rsp
        .cfi_def_cfa 7, 8
        ret
        .cfi_endproc

        .cfi_startproc
        test    edx, edx
        jle     .L7
        lea     eax, -1[rdx]
        cmp     eax, 2
        jbe     .L8
        mov     ecx, edx
        vxorpd  xmm7, xmm7, xmm7
        xor     eax, eax
        vmovapd xmm8, xmm7
        shr     ecx, 2
        vmovapd xmm9, xmm7
        vmovapd xmm6, xmm7
        sal     rcx, 5
        vmovapd xmm1, xmm7
        vmovapd xmm5, xmm7
        .p2align 4,,10
        .p2align 3
.L4:
        vmovupd ymm0, YMMWORD PTR [rdi+rax]
        vmovupd ymm10, YMMWORD PTR [rsi+rax]
        add     rax, 32
        vaddsd  xmm5, xmm0, xmm5
        vunpckhpd       xmm2, xmm0, xmm0
        vaddsd  xmm1, xmm10, xmm1
        vextractf128    xmm3, ymm0, 0x1
        vaddsd  xmm2, xmm2, xmm5
        vaddsd  xmm5, xmm3, xmm2
        vunpckhpd       xmm2, xmm10, xmm10
        vunpckhpd       xmm3, xmm3, xmm3
        vaddsd  xmm1, xmm1, xmm2
        vaddsd  xmm5, xmm5, xmm3
        vextractf128    xmm3, ymm10, 0x1
        vaddsd  xmm2, xmm1, xmm3
        vunpckhpd       xmm1, xmm3, xmm3
        vmulpd  ymm3, ymm10, ymm10
        vaddsd  xmm1, xmm2, xmm1
        vmulpd  ymm2, ymm0, ymm0
        vmulpd  ymm0, ymm0, ymm10
        vaddsd  xmm6, xmm6, xmm2
        vunpckhpd       xmm4, xmm2, xmm2
        vextractf128    xmm2, ymm2, 0x1
        vaddsd  xmm6, xmm6, xmm4
        vaddsd  xmm4, xmm9, xmm3
        vaddsd  xmm6, xmm6, xmm2
        vunpckhpd       xmm2, xmm2, xmm2
        vaddsd  xmm6, xmm6, xmm2
        vunpckhpd       xmm2, xmm3, xmm3
        vextractf128    xmm3, ymm3, 0x1
        vaddsd  xmm4, xmm4, xmm2
        vaddsd  xmm2, xmm8, xmm0
        vaddsd  xmm4, xmm4, xmm3
        vunpckhpd       xmm3, xmm3, xmm3
        vaddsd  xmm9, xmm4, xmm3
        vunpckhpd       xmm3, xmm0, xmm0
        vextractf128    xmm0, ymm0, 0x1
        vaddsd  xmm2, xmm2, xmm3
        vaddsd  xmm2, xmm2, xmm0
        vunpckhpd       xmm0, xmm0, xmm0
        vaddsd  xmm8, xmm2, xmm0
        cmp     rcx, rax
        jne     .L4
        mov     eax, edx
        and     eax, -4
        test    dl, 3
        je      .L16
        vzeroupper
.L3:
        movsx   rcx, eax
        vmovsd  xmm2, QWORD PTR [rdi+rcx*8]
        vmovsd  xmm0, QWORD PTR [rsi+rcx*8]
        lea     ecx, 1[rax]
        vfmadd231sd     xmm6, xmm2, xmm2
        vfmadd231sd     xmm9, xmm0, xmm0
        vaddsd  xmm5, xmm5, xmm2
        vfmadd231sd     xmm8, xmm2, xmm0
        vaddsd  xmm1, xmm1, xmm0
        cmp     edx, ecx
        jle     .L5
        movsx   rcx, ecx
        add     eax, 2
        vmovsd  xmm2, QWORD PTR [rdi+rcx*8]
        vmovsd  xmm0, QWORD PTR [rsi+rcx*8]
        vfmadd231sd     xmm6, xmm2, xmm2
        vfmadd231sd     xmm9, xmm0, xmm0
        vaddsd  xmm5, xmm5, xmm2
        vfmadd231sd     xmm8, xmm2, xmm0
        vaddsd  xmm1, xmm1, xmm0
        cmp     edx, eax
        jle     .L5
        cdqe
        vmovsd  xmm0, QWORD PTR [rdi+rax*8]
        vmovsd  xmm2, QWORD PTR [rsi+rax*8]
        vfmadd231sd     xmm6, xmm0, xmm0
        vfmadd231sd     xmm9, xmm2, xmm2
        vaddsd  xmm5, xmm5, xmm0
        vfmadd231sd     xmm8, xmm2, xmm0
        vaddsd  xmm1, xmm1, xmm2
.L5:
        vmulsd  xmm2, xmm1, xmm5
        vmulsd  xmm5, xmm5, xmm5
        vmulsd  xmm1, xmm1, xmm1
.L2:
        vxorps  xmm4, xmm4, xmm4
        vcvtsi2sd       xmm4, xmm4, edx
        vfmsub231sd     xmm2, xmm4, xmm8
        vfmsub132sd     xmm6, xmm5, xmm4
        vfmsub132sd     xmm4, xmm1, xmm9
        vmulsd  xmm4, xmm6, xmm4
        vucomisd        xmm7, xmm4
        vsqrtsd xmm8, xmm4, xmm4
        ja      .L17
        vdivsd  xmm0, xmm2, xmm8
        ret
        .p2align 4,,10
        .p2align 3
.L16:
        vzeroupper
        jmp     .L5
        .p2align 4,,10
        .p2align 3
.L7:
        vxorpd  xmm7, xmm7, xmm7
        vmovapd xmm1, xmm7
        vmovapd xmm5, xmm7
        vmovapd xmm2, xmm7
        vmovapd xmm8, xmm7
        vmovapd xmm9, xmm7
        vmovapd xmm6, xmm7
        jmp     .L2
.L8:
        vxorpd  xmm7, xmm7, xmm7
        xor     eax, eax
        vmovapd xmm8, xmm7
        vmovapd xmm9, xmm7
        vmovapd xmm6, xmm7
        vmovapd xmm1, xmm7
        vmovapd xmm5, xmm7
        jmp     .L3
.L17:
        lea     r10, 8[rsp]
        .cfi_def_cfa 10, 0
        and     rsp, -32
        vmovapd xmm0, xmm4
        push    QWORD PTR -8[r10]
        push    rbp
        mov     rbp, rsp
        .cfi_escape 0x10,0x6,0x2,0x76,0
        push    r10
        .cfi_escape 0xf,0x3,0x76,0x78,0x6
        sub     rsp, 40
        vmovsd  QWORD PTR -32[rbp], xmm2
        vmovsd  QWORD PTR -24[rbp], xmm8
        call    sqrt@PLT
        vmovsd  xmm2, QWORD PTR -32[rbp]
        vmovsd  xmm8, QWORD PTR -24[rbp]
        add     rsp, 40
        pop     r10
        .cfi_def_cfa 10, 0
        pop     rbp
        vdivsd  xmm0, xmm2, xmm8
        lea     rsp, -8[r10]
        .cfi_def_cfa 7, 8
        ret
        .cfi_endproc

        .cfi_startproc
        edx <= 0 : .L7
        eax = &[rdx-1]
        eax u<= 2 : .L8
        ecx = edx
        xmm7d v|=| 0
        eax = 0
        xmm8d v|=a| xmm7
        ecx u>>= 2
        xmm9d v|=a| xmm7
        xmm6d v|=a| xmm7
        rcx <<= 5
        xmm1d v|=a| xmm7
        xmm5d v|=a| xmm7
        .p2align 4,,10
        .p2align 3
.L4:
        ymm0d v|=u| 32bytes[rdi+rax]
        ymm10d v|=u| 32bytes[rsi+rax]
        rax += 32
        xmm5d v= xmm0 + xmm5
        xmm2d v|=| unpackhi(xmm0, xmm0)
        xmm1d v= xmm10 + xmm1
        xmm3d v|=| ymm0d[2:4]
        xmm2d v= xmm2 + xmm5
        xmm5d v= xmm3 + xmm2
        xmm2d v|=| unpackhi(xmm10, xmm10)
        xmm3d v|=| unpackhi(xmm3, xmm3)
        xmm1d v= xmm1 + xmm2
        xmm5d v= xmm5 + xmm3
        xmm3d v|=| ymm10d[2:4]
        xmm2d v= xmm1 + xmm3
        xmm1d v|=| unpackhi(xmm3, xmm3)
        ymm3d v|=| ymm10 * ymm10
        xmm1d v= xmm2 + xmm1
        ymm2d v|=| ymm0 * ymm0
        ymm0d v|=| ymm0 * ymm10
        xmm6d v= xmm6 + xmm2
        xmm4d v|=| unpackhi(xmm2, xmm2)
        xmm2d v|=| ymm2d[2:4]
        xmm6d v= xmm6 + xmm4
        xmm4d v= xmm9 + xmm3
        xmm6d v= xmm6 + xmm2
        xmm2d v|=| unpackhi(xmm2, xmm2)
        xmm6d v= xmm6 + xmm2
        xmm2d v|=| unpackhi(xmm3, xmm3)
        xmm3d v|=| ymm3d[2:4]
        xmm4d v= xmm4 + xmm2
        xmm2d v= xmm8 + xmm0
        xmm4d v= xmm4 + xmm3
        xmm3d v|=| unpackhi(xmm3, xmm3)
        xmm9d v= xmm4 + xmm3
        xmm3d v|=| unpackhi(xmm0, xmm0)
        xmm0d v|=| ymm0d[2:4]
        xmm2d v= xmm2 + xmm3
        xmm2d v= xmm2 + xmm0
        xmm0d v|=| unpackhi(xmm0, xmm0)
        xmm8d v= xmm2 + xmm0
        rcx != rax : .L4
        eax = edx
        eax &= -4
        dl <&> 3
        == : .L16
        vzeroupper
.L3:
        rcx = sx(eax)
        xmm2d v= 8bytes[rdi+rcx*8]
        xmm0d v= 8bytes[rsi+rcx*8]
        ecx = &[rax+1]
        xmm6d v= xmm2 * xmm2 + xmm6
        xmm9d v= xmm0 * xmm0 + xmm9
        xmm5d v= xmm5 + xmm2
        xmm8d v= xmm2 * xmm0 + xmm8
        xmm1d v= xmm1 + xmm0
        edx <= ecx : .L5
        rcx = sx(ecx)
        eax += 2
        xmm2d v= 8bytes[rdi+rcx*8]
        xmm0d v= 8bytes[rsi+rcx*8]
        xmm6d v= xmm2 * xmm2 + xmm6
        xmm9d v= xmm0 * xmm0 + xmm9
        xmm5d v= xmm5 + xmm2
        xmm8d v= xmm2 * xmm0 + xmm8
        xmm1d v= xmm1 + xmm0
        edx <= eax : .L5
        rax = sx(eax)
        xmm0d v= 8bytes[rdi+rax*8]
        xmm2d v= 8bytes[rsi+rax*8]
        xmm6d v= xmm0 * xmm0 + xmm6
        xmm9d v= xmm2 * xmm2 + xmm9
        xmm5d v= xmm5 + xmm0
        xmm8d v= xmm2 * xmm0 + xmm8
        xmm1d v= xmm1 + xmm2
.L5:
        xmm2d v= xmm1 * xmm5
        xmm5d v= xmm5 * xmm5
        xmm1d v= xmm1 * xmm1
.L2:
        xmm4s v|=| 0
        xmm4d v= float(edx)
        xmm2d v= xmm4 * xmm8 - xmm2
        xmm6d v= xmm6 * xmm4 - xmm5
        xmm4d v= xmm4 * xmm9 - xmm1
        xmm4d v= xmm6 * xmm4
        xmm7d vuo<=> xmm4d
        xmm8d v= sqrt(xmm4d[0]), xmm4d[1]
        u> : .L17
        xmm0d v= xmm2 / xmm8
        ret
        .p2align 4,,10
        .p2align 3
.L16:
        vzeroupper
        :.L5
        .p2align 4,,10
        .p2align 3
.L7:
        xmm7d v|=| 0
        xmm1d v|=a| xmm7
        xmm5d v|=a| xmm7
        xmm2d v|=a| xmm7
        xmm8d v|=a| xmm7
        xmm9d v|=a| xmm7
        xmm6d v|=a| xmm7
        :.L2
.L8:
        xmm7d v|=| 0
        eax = 0
        xmm8d v|=a| xmm7
        xmm9d v|=a| xmm7
        xmm6d v|=a| xmm7
        xmm1d v|=a| xmm7
        xmm5d v|=a| xmm7
        :.L3
.L17:
        r10 = &[rsp+8]
        .cfi_def_cfa 10, 0
        rsp &= -32
        xmm0d v|=a| xmm4
        push 8bytes[r10-8]
        push rbp
        rbp = rsp
        .cfi_escape 0x10,0x6,0x2,0x76,0
        push r10
        .cfi_escape 0xf,0x3,0x76,0x78,0x6
        rsp -= 40
        8bytes[rbp-32] v= xmm2d
        8bytes[rbp-24] v= xmm8d
        sqrt@PLT(...)
        xmm2d v= 8bytes[rbp-32]
        xmm8d v= 8bytes[rbp-24]
        rsp += 40
        pop r10
        .cfi_def_cfa 10, 0
        pop rbp
        xmm0d v= xmm2 / xmm8
        rsp = &[r10-8]
        .cfi_def_cfa 7, 8
        ret
        .cfi_endproc